{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdd4b3b-0bda-475c-a3ae-7ab9a3d9cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.saving import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, Embedding, add\n",
    "import re\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d8096-5978-4333-afb5-98ef4516766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========use the code or load directly from pickle from below cell\n",
    "model = InceptionV3()\n",
    "# restructure the model i.e. remove the last layer used for classification\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# preprocess image\n",
    "directory = os.path.join('d:/evoastra/','images')\n",
    "imagedict = {}\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    #load the image from file\n",
    "    img_path = directory + \"/\" + img_name\n",
    "    image = load_img(img_path,target_size=(299,299))\n",
    "    #convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    #reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = image / 255\n",
    "    # get image id\n",
    "    image_id = img_name.split('.')[0]\n",
    "    imagedict[image_id] = image\n",
    "\n",
    "#extract features from image\n",
    "features = {}\n",
    "for img_array in tqdm(imagedict):\n",
    "    #extract feature\n",
    "    feature = model.predict(imagedict[img_array], verbose=0)\n",
    "    #store the features\n",
    "    features[img_array] = feature\n",
    "\n",
    "pickle.dump(features,open('./feaatures.pkl','wb'))\n",
    "#======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac85334-fd88-4c10-9e70-85d1da384879",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pickle.load(open('./feaatures.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f780ef-909e-4a80-8bdc-19013ec7f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========use the code or load directly from pickle from below cell\n",
    "with open('./captions.txt','r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()\n",
    "\n",
    "#create mapping of image to captions\n",
    "mapping = {}\n",
    "#process lines\n",
    "for lines in tqdm(captions_doc.split('\\n')):\n",
    "    #split the line by comma\n",
    "    tokens = lines.split(',')\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    #remove file extension from image_id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    #convert caption list to string\n",
    "    caption = ' '.join(caption)\n",
    "    #create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    #store the caption\n",
    "    mapping[image_id].append(caption)     \n",
    "\n",
    "pickle.dump(mapping,open('mappings.pkl','wb'))\n",
    "#======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcd9f47-072d-484d-a5f2-2f35444ed938",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pickle.load(open('mappings.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd11af-b243-4782-bc87-1ab66b488d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========use the code or load directly from pickle from below cell\n",
    "def clean_text(mapping):\n",
    "    for key,captions in tqdm(mapping.items()):\n",
    "        for i in range(len(captions)):\n",
    "            # tke one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lower case \n",
    "            caption = caption.lower()\n",
    "            # expand contractions\n",
    "            caption = contractions.fix(caption)\n",
    "            # delete special charectors, digits and others\n",
    "            #caption = caption.replace('[^a-z]','')\n",
    "            caption = re.sub(r'[^a-z\\s]','',caption)\n",
    "            # replacing multiple space with one space\n",
    "            #caption = caption.replace('\\s+',' ')\n",
    "            caption = re.sub(r'\\s+',' ',caption)\n",
    "            # remove one charector word\n",
    "            caption = ' '.join([word for word in caption.split() if len(word)>1])\n",
    "            # add start and end\n",
    "            caption = 'start ' + caption + ' end'\n",
    "            captions[i] = caption\n",
    "\n",
    "cleaned_corpus = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        cleaned_corpus.append(caption)\n",
    "\n",
    "# tokenize the text\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(cleaned_corpus)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "maxlen = max(len(caption.split()) for caption in cleaned_corpus)\n",
    "\n",
    "pickle.dump(t,open('tokenizer.pkl','wb'))\n",
    "\n",
    "#Tokenize the captions\n",
    "embedded_mapping ={}\n",
    "for key,captions in mapping.items():\n",
    "    for i in range(len(captions)):\n",
    "        tokenized_caption = t.texts_to_sequences([captions[i]])[0]\n",
    "        padded_tokenized_caption = pad_sequences([tokenized_caption],maxlen,padding='post')[0]    #0 to eliminate outside []\n",
    "        #create list if needed\n",
    "        if key not in embedded_mapping:\n",
    "            embedded_mapping[key] = []\n",
    "        #store the caption\n",
    "        embedded_mapping[key].append(padded_tokenized_caption)\n",
    "\n",
    "pickle.dump(embedded_mapping,open('./processed_captions.pkl','wb'))\n",
    "\n",
    "caption_prop = {}\n",
    "caption_prop['vocab size'] = vocab_size\n",
    "caption_prop['max length'] = maxlen\n",
    "pickle.dump(caption_prop,open('./captions_properties.pkl','wb'))\n",
    "#======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7357039-bca7-436f-993b-62ccde327614",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pickle.load(open('tokenizer.pkl','rb'))\n",
    "embedded_mapping = pickle.load(open('./processed_captions.pkl','rb'))\n",
    "caption_prop = pickle.load(open('./captions_properties.pkl','rb'))\n",
    "vocab_size = caption_prop['vocab size']\n",
    "maxlen = caption_prop['max length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f914e40-a170-4c8b-96e0-e62109691238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequence generation\n",
    "image_ids = list(embedded_mapping.keys())\n",
    "\n",
    "# create data generator to get data in batch\n",
    "def data_generator(data_keys, embedded_mapping, features, max_length, vocab_size, batch_size):\n",
    "    #data_keys will be image_ids\n",
    "    #loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n=n+1\n",
    "            sequences = embedded_mapping[key]\n",
    "            #process each caption\n",
    "            for seq in sequences:\n",
    "                #split the sequence in X,y pairs\n",
    "                for i in range(1,len(seq)):\n",
    "                    #stop when padding starts\n",
    "                    if seq[i]==0:\n",
    "                        break\n",
    "                    #split into input and output pairs\n",
    "                    input_seq, output_seq = seq[:i], seq[i]\n",
    "                    #pad input sequence\n",
    "                    input_seq = pad_sequences([input_seq],maxlen=max_length,padding='post')[0]\n",
    "                    #encode output sequence\n",
    "                    output_seq = to_categorical(output_seq,num_classes=vocab_size)\n",
    "                    #store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(input_seq)\n",
    "                    y.append(output_seq)       \n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                #yield [X1,X2], y\n",
    "                yield (X1,X2), y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137fb2e7-23db-4f13-9092-8bc9e2373604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 12s/step - loss: 6.4276\n"
     ]
    }
   ],
   "source": [
    "#Model creation\n",
    "#encoder model\n",
    "#image feature layer\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256,activation='relu')(fe1)\n",
    "\n",
    "#sequence feature layer\n",
    "inputs2 = Input(shape=(maxlen,))\n",
    "se1 = Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "#decoder model\n",
    "decoder1 = add([fe2,se3])\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# train test split\n",
    "image_ids = list(embedded_mapping.keys())\n",
    "split = int(len(image_ids) * .8)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]\n",
    "\n",
    "#train the model\n",
    "def train_model(model,epoch):\n",
    "    epochs = epoch\n",
    "    batch_size = 64\n",
    "    steps = len(train) // batch_size\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        #create data generator\n",
    "        generator = data_generator(train,embedded_mapping,features,maxlen,vocab_size,batch_size)\n",
    "        #fit for each epoch\n",
    "        model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "train_model(model,1)\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a67887-d6ad-4d78-9173-e4a57ca12f32",
   "metadata": {},
   "outputs": [],
   "source": [
    " # run this code to retrain model till 13 epochs for getting .56 Bleu score\n",
    "model = load_model(\"model.keras\")\n",
    "train_model(model,6)\n",
    "model.save('model_7.keras')\n",
    "model = load_model(\"model_7.keras\")\n",
    "train_model(model,6)\n",
    "model.save('model_13.keras')       #final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bb050-3558-47c6-b738-6c625ff76f5b",
   "metadata": {},
   "source": [
    "#### Proceed with testing and hyper parameter tuning iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1cf19-b7f8-48cf-aaa1-822ec70549f7",
   "metadata": {},
   "source": [
    "## Testing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec45ee-59cf-47c6-8cd0-5b5584c6816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "#Generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    #add start tag\n",
    "    in_text = 'start'\n",
    "    #iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        #encode the input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #pad the sequence\n",
    "        sequence = pad_sequences([sequence],max_length,padding='post')\n",
    "        #predict next word\n",
    "        yhat = loaded_model.predict([image, sequence], verbose=0)\n",
    "        #get index with highest probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        #convert index to word\n",
    "        word = idx_to_word(yhat,tokenizer)\n",
    "        #stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        #append word as input for generating next word\n",
    "        in_text += ' ' + word\n",
    "        #stop if we have reached the end tag\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in test:\n",
    "    #get the actual caption\n",
    "    captions = mapping[key]\n",
    "    #predict the caption for image\n",
    "    y_pred = predict_caption(loaded_model, features[key], t, maxlen)\n",
    "    #split into words\n",
    "    actual_caption = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    #append the list\n",
    "    actual.append(actual_caption)\n",
    "    predicted.append(y_pred)\n",
    "\n",
    "#calculate BLEU score\n",
    "print('BLEU 1 : %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU 2 : %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
